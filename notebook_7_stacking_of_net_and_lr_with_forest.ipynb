{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:05.787068Z",
     "iopub.status.busy": "2022-02-20T10:05:05.786609Z",
     "iopub.status.idle": "2022-02-20T10:05:13.525363Z",
     "shell.execute_reply": "2022-02-20T10:05:13.524540Z",
     "shell.execute_reply.started": "2022-02-20T10:05:05.786975Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:13.527282Z",
     "iopub.status.busy": "2022-02-20T10:05:13.527008Z",
     "iopub.status.idle": "2022-02-20T10:05:16.674763Z",
     "shell.execute_reply": "2022-02-20T10:05:16.674027Z",
     "shell.execute_reply.started": "2022-02-20T10:05:13.527245Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "stopwords = stopwords.words('russian')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:16.676327Z",
     "iopub.status.busy": "2022-02-20T10:05:16.676066Z",
     "iopub.status.idle": "2022-02-20T10:05:16.682873Z",
     "shell.execute_reply": "2022-02-20T10:05:16.682148Z",
     "shell.execute_reply.started": "2022-02-20T10:05:16.676294Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    cleanup()\n",
    "    \n",
    "    y_pred, y_true = eval_preds\n",
    "    \n",
    "    if type(y_pred) == tuple:\n",
    "        y_pred = y_pred[0]\n",
    "    \n",
    "    z = 1 / (1 + np.exp(-y_pred))\n",
    "    y_pred, y_true = np.array(z >= 0.5, dtype=int), y_true.astype(int)\n",
    "    \n",
    "    return {'f1_score': f1_score(y_true, y_pred, average='samples')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:16.686363Z",
     "iopub.status.busy": "2022-02-20T10:05:16.685538Z",
     "iopub.status.idle": "2022-02-20T10:05:16.694802Z",
     "shell.execute_reply": "2022-02-20T10:05:16.694122Z",
     "shell.execute_reply.started": "2022-02-20T10:05:16.686323Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDatasetForClassification(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y=None, tokenizer=None):\n",
    "        self.sentences = list(X)\n",
    "        self.labels = torch.FloatTensor(y) if y is not None else torch.zeros((len(self.sentences), 9))\n",
    "        \n",
    "        self.tokenizer_outputs = tokenizer.batch_encode_plus(self.sentences, return_tensors=\"pt\", \n",
    "                                                             max_length=128, padding=True, truncation=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_ids = self.tokenizer_outputs['input_ids'][index]\n",
    "        attention_mask = self.tokenizer_outputs['attention_mask'][index]\n",
    "\n",
    "        label = self.labels[index]\n",
    "\n",
    "        return {'attention_mask': attention_mask, \n",
    "                'input_ids': input_ids, \n",
    "                'labels': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:16.696661Z",
     "iopub.status.busy": "2022-02-20T10:05:16.695984Z",
     "iopub.status.idle": "2022-02-20T10:05:16.729303Z",
     "shell.execute_reply": "2022-02-20T10:05:16.728504Z",
     "shell.execute_reply.started": "2022-02-20T10:05:16.696607Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:16.730935Z",
     "iopub.status.busy": "2022-02-20T10:05:16.730659Z",
     "iopub.status.idle": "2022-02-20T10:05:16.906410Z",
     "shell.execute_reply": "2022-02-20T10:05:16.905574Z",
     "shell.execute_reply.started": "2022-02-20T10:05:16.730898Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:16.908324Z",
     "iopub.status.busy": "2022-02-20T10:05:16.907515Z",
     "iopub.status.idle": "2022-02-20T10:05:19.370887Z",
     "shell.execute_reply": "2022-02-20T10:05:19.370083Z",
     "shell.execute_reply.started": "2022-02-20T10:05:16.908283Z"
    }
   },
   "outputs": [],
   "source": [
    "Xy_train_val = pd.read_csv('../input/headhunter/data/data/train.csv', index_col='review_id'\n",
    "                          ).fillna('Нет информации.')\n",
    "X_train_val, y_train_val = Xy_train_val.iloc[:, :-1], Xy_train_val.iloc[:, -1]\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[str(i) for i in range(9)])\n",
    "y_train_val = mb.fit_transform(y_train_val)\n",
    "\n",
    "X_test = pd.read_csv('../input/headhunter/data/data/test.csv', index_col='review_id'\n",
    "                    ).fillna('Нет информации.')\n",
    "\n",
    "X_train_val = X_train_val.iloc[:, 2:4].apply(lambda row: '. '.join(row.values.astype(str)), axis=1).values\n",
    "X_test = X_test.iloc[:, 2:4].apply(lambda row: '. '.join(row.values.astype(str)), axis=1).values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:19.372662Z",
     "iopub.status.busy": "2022-02-20T10:05:19.372270Z",
     "iopub.status.idle": "2022-02-20T10:05:31.748857Z",
     "shell.execute_reply": "2022-02-20T10:05:31.748081Z",
     "shell.execute_reply.started": "2022-02-20T10:05:19.372602Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = 'DeepPavlov/rubert-base-cased-sentence' # 0.780006575919\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    base_model, num_labels=9, problem_type='multi_label_classification'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:31.750267Z",
     "iopub.status.busy": "2022-02-20T10:05:31.749995Z",
     "iopub.status.idle": "2022-02-20T10:05:53.448541Z",
     "shell.execute_reply": "2022-02-20T10:05:53.447760Z",
     "shell.execute_reply.started": "2022-02-20T10:05:31.750230Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_train = MyDatasetForClassification(X_train, y_train, tokenizer)\n",
    "dataset_val = MyDatasetForClassification(X_val[:500], y_val[:500], tokenizer)\n",
    "dataset_test = MyDatasetForClassification(X_test, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:05:53.449974Z",
     "iopub.status.busy": "2022-02-20T10:05:53.449729Z",
     "iopub.status.idle": "2022-02-20T10:25:32.554326Z",
     "shell.execute_reply": "2022-02-20T10:25:32.553465Z",
     "shell.execute_reply.started": "2022-02-20T10:05:53.449940Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanup()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    per_device_train_batch_size=16, #\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2, # \n",
    "    warmup_steps=1000, #\n",
    "    learning_rate=1e-5, #\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    eval_accumulation_steps=10,\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model='f1_score',\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                  args=training_args, \n",
    "                  train_dataset=dataset_train, \n",
    "                  eval_dataset=dataset_val, \n",
    "                  compute_metrics=compute_metrics)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "torch.save(model.state_dict(), 'model_bert_pavlov_final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:25:32.557560Z",
     "iopub.status.busy": "2022-02-20T10:25:32.557300Z",
     "iopub.status.idle": "2022-02-20T10:25:34.843129Z",
     "shell.execute_reply": "2022-02-20T10:25:34.842381Z",
     "shell.execute_reply.started": "2022-02-20T10:25:32.557523Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_val = MyDatasetForClassification(X_val, y_val, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:25:34.844699Z",
     "iopub.status.busy": "2022-02-20T10:25:34.844438Z",
     "iopub.status.idle": "2022-02-20T10:31:49.306262Z",
     "shell.execute_reply": "2022-02-20T10:31:49.305538Z",
     "shell.execute_reply.started": "2022-02-20T10:25:34.844662Z"
    }
   },
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=False)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=64, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=False)\n",
    "\n",
    "outputs_train, outputs_val, outputs_test = [], [], []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dataloader_train):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs_train.append(model(**batch).logits.cpu().numpy())\n",
    "        \n",
    "    X_train_new_1 = np.concatenate(outputs_train, axis=0)\n",
    "        \n",
    "    for i, batch in enumerate(dataloader_val):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs_val.append(model(**batch).logits.cpu().numpy())\n",
    "    \n",
    "    X_val_new_1 = np.concatenate(outputs_val, axis=0)\n",
    "    \n",
    "    for i, batch in enumerate(dataloader_test):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs_test.append(model(**batch).logits.cpu().numpy())\n",
    "    \n",
    "    X_test_new_1 = np.concatenate(outputs_test, axis=0)\n",
    "\n",
    "X_train_new_1.shape, X_val_new_1.shape, X_test_new_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:31:49.307618Z",
     "iopub.status.busy": "2022-02-20T10:31:49.307376Z",
     "iopub.status.idle": "2022-02-20T10:31:49.324655Z",
     "shell.execute_reply": "2022-02-20T10:31:49.318983Z",
     "shell.execute_reply.started": "2022-02-20T10:31:49.307579Z"
    }
   },
   "outputs": [],
   "source": [
    "class DummyTransformer(TransformerMixin):\n",
    "    \"\"\"\n",
    "    Mini class to return initial features without transformation\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, value=None):\n",
    "        TransformerMixin.__init__(self)\n",
    "        self.value = value\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return {'value': self.value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:31:58.798519Z",
     "iopub.status.busy": "2022-02-20T10:31:58.798194Z",
     "iopub.status.idle": "2022-02-20T10:35:57.158311Z",
     "shell.execute_reply": "2022-02-20T10:35:57.157498Z",
     "shell.execute_reply.started": "2022-02-20T10:31:58.798483Z"
    }
   },
   "outputs": [],
   "source": [
    "Xy_train_val = pd.read_csv('../input/headhunter/data/data/train.csv', index_col='review_id').fillna('Unknown')\n",
    "X_train_val, y_train_val = Xy_train_val.iloc[:, :-1], Xy_train_val.iloc[:, -1] # .apply(lambda x: int(x[0]))\n",
    "\n",
    "mb = MultiLabelBinarizer(classes=[str(i) for i in range(9)])\n",
    "y_train_val = mb.fit_transform(y_train_val)\n",
    "\n",
    "X_test = pd.read_csv('../input/headhunter/data/data/test.csv', index_col='review_id').fillna('Unknown')\n",
    "\n",
    "for data in [X_train_val, X_test]:\n",
    "    \n",
    "    # class 0: special symbol\n",
    "    data['xa_symbol_pos'] = (data['positive'].str.find('\\xa0') != -1).astype(int)\n",
    "    data['xa_symbol_neg'] = (data['negative'].str.find('\\xa0') != -1).astype(int)\n",
    "    \n",
    "    # small preprocessing\n",
    "    data['positive'] = data['positive'].str.replace(',', ', '\n",
    "                                                   ).str.replace('.', '. '\n",
    "                                                                ).apply(lambda x: re.sub(' +', ' ', x))\n",
    "    data['negative'] = data['negative'].str.replace(',', ', '\n",
    "                                                   ).str.replace('.', '. '\n",
    "                                                                ).apply(lambda x: re.sub(' +', ' ', x))\n",
    "    \n",
    "    # class 8: length (woith round -1)\n",
    "    data['length_pos'] = data['positive'].apply(lambda x: round(len(x), -1)) # .str.len() also works\n",
    "    data.loc[data['length_pos'] > 1000, 'length_pos'] = 1000\n",
    "    data['length_neg'] = data['negative'].apply(lambda x: round(len(x), -1))\n",
    "    data.loc[data['length_neg'] > 1000, 'length_neg'] = 1000\n",
    "    \n",
    "    # class \n",
    "    data['max_pos'] = data['positive'].apply(lambda x: np.max([len(w) for w in x.split(' ')]))\n",
    "    data.loc[data['max_pos'] > 25, 'max_pos'] = 25\n",
    "    data['max_neg'] = data['negative'].apply(lambda x: np.max([len(w) for w in x.split(' ')]))\n",
    "    data.loc[data['max_neg'] > 25, 'max_neg'] = 25\n",
    "    \n",
    "    # class \n",
    "    data['most_common_pos'] = data['positive'].apply(\n",
    "        lambda x: Counter([w for w in x.split(' ')]).most_common(1)[0][1]\n",
    "    )\n",
    "    data.loc[data['most_common_pos'] > 25, 'most_common_pos'] = 25\n",
    "    data['most_common_neg'] = data['negative'].apply(\n",
    "        lambda x: Counter([w for w in x.split(' ')]).most_common(1)[0][1]\n",
    "    )\n",
    "    data.loc[data['most_common_neg'] > 25, 'most_common_neg'] = 25\n",
    "    \n",
    "    for col in ['city', 'position']:\n",
    "        counts = data[col].value_counts()\n",
    "        data.loc[data[col].isin(counts[counts < 5].index), col] = 'Прочее'\n",
    "        \n",
    "    cols = ['salary_rating', 'team_rating', 'managment_rating', \n",
    "            'career_rating', 'workplace_rating', 'rest_recovery_rating']\n",
    "    \n",
    "    for i in range(1, 5+1):\n",
    "        data[f'count_{i}'] = (data.loc[:, cols] == i).sum(axis=1)\n",
    "    \n",
    "    data['rating_mean'] = data.loc[:, cols].mean(axis=1)\n",
    "    data['rating_std'] = data.loc[:, cols].std(axis=1)\n",
    "    \n",
    "    data['positive_stem'] = data['positive'].apply(\n",
    "        lambda x: re.sub(' +', ' ', ' '.join(\n",
    "            [stemmer.stem(word) if word not in string.punctuation else '' for word in word_tokenize(x)]\n",
    "        ))\n",
    "    )\n",
    "    data['negative_stem'] = data['negative'].apply(\n",
    "        lambda x: re.sub(' +', ' ', ' '.join(\n",
    "            [stemmer.stem(word) if word not in string.punctuation else '' for word in word_tokenize(x)]\n",
    "        ))\n",
    "    )\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:35:57.160310Z",
     "iopub.status.busy": "2022-02-20T10:35:57.159982Z",
     "iopub.status.idle": "2022-02-20T10:35:57.256314Z",
     "shell.execute_reply": "2022-02-20T10:35:57.255658Z",
     "shell.execute_reply.started": "2022-02-20T10:35:57.160271Z"
    }
   },
   "outputs": [],
   "source": [
    "set1 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=3, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '2', 'positive_stem']).vocabulary_.keys()\n",
    "\n",
    "set2 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=3, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '4', 'positive_stem']).vocabulary_.keys()\n",
    "\n",
    "set3 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=5, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '5', 'positive_stem']).vocabulary_.keys()\n",
    "\n",
    "set4 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=10, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '7', 'positive_stem']).vocabulary_.keys()\n",
    "\n",
    "set_words_pos = set(set1).union(set(set2)).union(set(set3)).union(set(set4))\n",
    "\n",
    "len(set1), len(set2), len(set3), len(set4), len(set_words_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:35:57.257690Z",
     "iopub.status.busy": "2022-02-20T10:35:57.257441Z",
     "iopub.status.idle": "2022-02-20T10:35:57.354347Z",
     "shell.execute_reply": "2022-02-20T10:35:57.353515Z",
     "shell.execute_reply.started": "2022-02-20T10:35:57.257652Z"
    }
   },
   "outputs": [],
   "source": [
    "set1 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=3, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '2', 'negative_stem']).vocabulary_.keys()\n",
    "\n",
    "set2 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=3, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '4', 'negative_stem']).vocabulary_.keys()\n",
    "\n",
    "set3 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=5, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '5', 'negative_stem']).vocabulary_.keys()\n",
    "\n",
    "set4 = TfidfVectorizer(ngram_range=(1, 3), max_df=1.0, min_df=10, stop_words=stopwords, analyzer='word'\n",
    "                      ).fit(X_train_val.loc[Xy_train_val.target == '7', 'negative_stem']).vocabulary_.keys()\n",
    "\n",
    "set_words_neg = set(set1).union(set(set2)).union(set(set3)).union(set(set4))\n",
    "\n",
    "len(set1), len(set2), len(set3), len(set4), len(set_words_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:35:57.357092Z",
     "iopub.status.busy": "2022-02-20T10:35:57.356809Z",
     "iopub.status.idle": "2022-02-20T10:39:53.104467Z",
     "shell.execute_reply": "2022-02-20T10:39:53.102917Z",
     "shell.execute_reply.started": "2022-02-20T10:35:57.357056Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('transforms', ColumnTransformer([\n",
    "        ('ohe', OneHotEncoder(handle_unknown='ignore'), [0, 1] + [*range(4, X_val.shape[1]-4)]),\n",
    "        ('two_features', DummyTransformer(), [X_val.shape[1]-4, X_val.shape[1]-3]),\n",
    "        ('tfidf1', TfidfVectorizer(ngram_range=(1, 4), max_df=0.999, min_df=0.001, \n",
    "                                   analyzer='char_wb'), 2),\n",
    "        ('tfidf2', TfidfVectorizer(ngram_range=(1, 4), max_df=0.999, min_df=0.001, \n",
    "                                   analyzer='char_wb'), 3),\n",
    "        ('count1', CountVectorizer(ngram_range=(1, 4), max_df=0.999, min_df=0.001, binary=True,\n",
    "                                   analyzer='char_wb'), 2),\n",
    "        ('count2', CountVectorizer(ngram_range=(1, 4), max_df=0.999, min_df=0.001, binary=True,\n",
    "                                   analyzer='char_wb'), 3),\n",
    "        ('count3', CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=True,\n",
    "                                   stop_words=stopwords, vocabulary=set_words_pos), X_val.shape[1]-2),\n",
    "        ('count4', CountVectorizer(ngram_range=(1, 3), analyzer='word', binary=True,\n",
    "                                   stop_words=stopwords, vocabulary=set_words_neg), X_val.shape[1]-1),\n",
    "    ])),\n",
    "    ('lr', OneVsRestClassifier(LogisticRegression(C=0.01, max_iter=500, n_jobs=-1, random_state=42)))\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(pipeline, 'model_logreg_final')\n",
    "\n",
    "print(round(f1_score(y_val, pipeline.predict(X_val), average='samples'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:39:53.106629Z",
     "iopub.status.busy": "2022-02-20T10:39:53.106359Z",
     "iopub.status.idle": "2022-02-20T10:42:07.950734Z",
     "shell.execute_reply": "2022-02-20T10:42:07.949843Z",
     "shell.execute_reply.started": "2022-02-20T10:39:53.106588Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_new_2 = pipeline.predict_proba(X_train)\n",
    "X_val_new_2 = pipeline.predict_proba(X_val)\n",
    "X_test_new_2 = pipeline.predict_proba(X_test)\n",
    "\n",
    "X_train_new_2.shape, X_val_new_2.shape, X_test_new_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:42:07.952515Z",
     "iopub.status.busy": "2022-02-20T10:42:07.952231Z",
     "iopub.status.idle": "2022-02-20T10:42:07.965614Z",
     "shell.execute_reply": "2022-02-20T10:42:07.964697Z",
     "shell.execute_reply.started": "2022-02-20T10:42:07.952475Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_new = np.concatenate([X_train_new_1, X_train_new_2], axis=1)\n",
    "X_val_new = np.concatenate([X_val_new_1, X_val_new_2], axis=1)\n",
    "X_test_new = np.concatenate([X_test_new_1, X_test_new_2], axis=1)\n",
    "\n",
    "X_train_new.shape, X_val_new.shape, X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:48:38.297207Z",
     "iopub.status.busy": "2022-02-20T10:48:38.296724Z",
     "iopub.status.idle": "2022-02-20T10:52:51.897255Z",
     "shell.execute_reply": "2022-02-20T10:52:51.896486Z",
     "shell.execute_reply.started": "2022-02-20T10:48:38.297163Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in range(3, 8):\n",
    "    print(d)\n",
    "    model = OneVsRestClassifier(RandomForestClassifier(max_depth=d, n_jobs=-1, random_state=42))\n",
    "    model.fit(X_train_new, y_train)\n",
    "    print('Train: ', round(f1_score(y_train, model.predict(X_train_new), average='samples'), 3))\n",
    "    print('Valid: ', round(f1_score(y_val, model.predict(X_val_new), average='samples'), 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:52:51.899942Z",
     "iopub.status.busy": "2022-02-20T10:52:51.899663Z",
     "iopub.status.idle": "2022-02-20T10:53:52.796800Z",
     "shell.execute_reply": "2022-02-20T10:53:52.796083Z",
     "shell.execute_reply.started": "2022-02-20T10:52:51.899901Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in range(3, 8):\n",
    "    print(d)\n",
    "    model = OneVsRestClassifier(ExtraTreesClassifier(max_depth=d, n_jobs=-1, random_state=42))\n",
    "    model.fit(X_train_new, y_train)\n",
    "    print('Train: ', round(f1_score(y_train, model.predict(X_train_new), average='samples'), 3))\n",
    "    print('Valid: ', round(f1_score(y_val, model.predict(X_val_new), average='samples'), 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T10:56:13.560157Z",
     "iopub.status.busy": "2022-02-20T10:56:13.559278Z",
     "iopub.status.idle": "2022-02-20T10:57:21.961186Z",
     "shell.execute_reply": "2022-02-20T10:57:21.960332Z",
     "shell.execute_reply.started": "2022-02-20T10:56:13.560104Z"
    }
   },
   "outputs": [],
   "source": [
    "for d in range(6, 11):\n",
    "    print(d)\n",
    "    model = OneVsRestClassifier(ExtraTreesClassifier(max_depth=d, bootstrap=True, n_jobs=-1, random_state=42))\n",
    "    model.fit(X_train_new, y_train)\n",
    "    print('Train: ', round(f1_score(y_train, model.predict(X_train_new), average='samples'), 3))\n",
    "    print('Valid: ', round(f1_score(y_val, model.predict(X_val_new), average='samples'), 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T11:03:26.106551Z",
     "iopub.status.busy": "2022-02-20T11:03:26.106268Z",
     "iopub.status.idle": "2022-02-20T11:06:37.098107Z",
     "shell.execute_reply": "2022-02-20T11:06:37.097305Z",
     "shell.execute_reply.started": "2022-02-20T11:03:26.106518Z"
    }
   },
   "outputs": [],
   "source": [
    "for n in [100, 150, 500]:\n",
    "    for d in [7, 10]:\n",
    "        print(n, d)\n",
    "        model = OneVsRestClassifier(ExtraTreesClassifier(n_estimators=n, max_depth=d, bootstrap=True, \n",
    "                                                         n_jobs=-1, random_state=42))\n",
    "        model.fit(X_train_new, y_train)\n",
    "        print('Train: ', round(f1_score(y_train, model.predict(X_train_new), average='samples'), 3))\n",
    "        print('Valid: ', round(f1_score(y_val, model.predict(X_val_new), average='samples'), 3))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T11:07:17.150520Z",
     "iopub.status.busy": "2022-02-20T11:07:17.149496Z",
     "iopub.status.idle": "2022-02-20T11:07:17.162212Z",
     "shell.execute_reply": "2022-02-20T11:07:17.161281Z",
     "shell.execute_reply.started": "2022-02-20T11:07:17.150452Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_val_new = np.concatenate([X_train_new, X_val_new], axis=0)\n",
    "y_train_val_new = np.concatenate([y_train, y_val], axis=0)\n",
    "\n",
    "X_train_val_new.shape, y_train_val_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T11:07:54.291488Z",
     "iopub.status.busy": "2022-02-20T11:07:54.290923Z",
     "iopub.status.idle": "2022-02-20T11:08:52.266173Z",
     "shell.execute_reply": "2022-02-20T11:08:52.265445Z",
     "shell.execute_reply.started": "2022-02-20T11:07:54.291446Z"
    }
   },
   "outputs": [],
   "source": [
    "model = OneVsRestClassifier(ExtraTreesClassifier(n_estimators=500, max_depth=7, bootstrap=True, \n",
    "                                                 n_jobs=-1, random_state=42))\n",
    "model.fit(X_train_val_new, y_train_val_new)\n",
    "\n",
    "joblib.dump(model, 'model_forest_final')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T11:08:52.268227Z",
     "iopub.status.busy": "2022-02-20T11:08:52.267972Z",
     "iopub.status.idle": "2022-02-20T11:08:52.274183Z",
     "shell.execute_reply": "2022-02-20T11:08:52.273385Z",
     "shell.execute_reply.started": "2022-02-20T11:08:52.268191Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_multilabel(model, X):\n",
    "    y_pred = list(map(lambda x: ','.join(x), mb.inverse_transform(model.predict(X))))\n",
    "    y_pred_top1 = model.predict_proba(X).argmax(axis=1)\n",
    "    \n",
    "    return np.where([len(x) > 0 for x in y_pred], y_pred, y_pred_top1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-20T11:08:52.275950Z",
     "iopub.status.busy": "2022-02-20T11:08:52.275614Z",
     "iopub.status.idle": "2022-02-20T11:09:14.666370Z",
     "shell.execute_reply": "2022-02-20T11:09:14.665638Z",
     "shell.execute_reply.started": "2022-02-20T11:08:52.275913Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'review_id': X_test.index, \n",
    "    'target': predict_multilabel(model, X_test_new)\n",
    "}).to_csv('answers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
